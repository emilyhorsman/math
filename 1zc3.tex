\documentclass[12pt]{article}
\renewcommand{\baselinestretch}{1.2}
\usepackage[letterpaper, margin=0.5in]{geometry}
\usepackage{amsmath}
\DeclareMathSizes{12}{13}{9.5}{9}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\tr}{tr}

\begin{document}


\section{Definitions}


\subsection{Row Echelon Form}

Row Echelon Form:
\begin{itemize}
    \item A nonzero row must have a leftmost ``leading'' 1. $\begin{bmatrix}0 1 0 0\end{bmatrix}$
    \item A nonzero row below another nonzero row must have its leading 1 farther to the right.
    \item Any zero rows must be grouped together at the bottom of the matrix.
    \item This form is not unique.
\end{itemize}
$$
\begin{bmatrix}
    0 & 1 & 2 & 6 & 0\\
    0 & 0 & 1 & -1 & 0\\
    0 & 0 & 0 & 0 & 1\\
    0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0
\end{bmatrix}
$$
Reduced Row Echelon Form:
\begin{itemize}
    \item Any column with a leading 1 must be zero elsewhere.
    \item This form is unique for any system.
\end{itemize}
$$
\begin{bmatrix}
    0 & 1 & 0 & 6 & 0\\
    0 & 0 & 1 & -1 & 0\\
    0 & 0 & 0 & 0 & 1\\
    0 & 0 & 0 & 0 & 0\\
    0 & 0 & 0 & 0 & 0
\end{bmatrix}
$$


\subsection{Pivot Positions and Columns}

The position of a leading 1 is a pivot position of its matrix. Columns with a pivot position are pivot columns.


\subsection{Leading and Free Variables}

Leading: Corresponding to a leading 1 in an augmented matrix.\\
Free: The remaining variables. Can be assigned a parameter.


\subsection{Trivial Solution}

A zero vector. If $Ax = 0$ has only the trivial solution then $x$ must be something like,
$$
\begin{bmatrix}
    x_1\\
    x_2\\
    \vdots\\
    x_r
\end{bmatrix}
=
\begin{bmatrix}
    0\\
    0\\
    \vdots\\
    0
\end{bmatrix}
$$


\subsection{Homogeneous System}

A matrix equation equal to a zero vector. All constant terms are 0. $Ax = 0$. A homogeneous system must be consistent. It will have either only the trivial solution or will have infinitely many solutions.


\subsection{Consistent}

A system is consistent if it has one or infinitely many solutions. There is no other option for a consistent system. An inconsistent system has no solutions.\\
A single linear equation with two or more unknowns must have infinitely many solutions.


\subsection{Symmetric Matrix}

A square matrix $A$ where $A = A^T$. Thus, $(A)_{ij} = (A)_{ji}$.
$$
\begin{bmatrix}1 & 9\\9 & 2\end{bmatrix}
$$


\subsection{Skew-symmetric Matrix}

A square matrix $A$ where $A^T = -A$.\\
All the main diagonal entries must be 0.
\begin{align*}
    -(A_{ij}) &= (A^T)_{ij}\\
    -(A_{ij}) &= A_{ji}\\
    -(A_{ii}) &= A_{ii}&\text{On the diagonal, }i = j\\
    A_{ii} &= 0&\text{0 is the only value that will hold}
\end{align*}


\subsection{Linear Combination}

The sum of multiple, equally sized matrices with multiple scalar coefficients can be expressed as $c_1A_1 + c_2A_2 + \dots + c_rA_r$.\\
This can be used to express matrix products. $A$ is an $m \times n$ matrix and $x$ is an $n \times 1$ column vector.
$$
Ax =
\begin{bmatrix}
    a_{11}x_1 + \dots + a_{1n}x_n\\
    \vdots\\
    a_{m1}x_1 + \dots + a_{mn}x_n
\end{bmatrix}
=
x_1
\begin{bmatrix}
    a_{11}\\
    a_{m1}
\end{bmatrix}
+ \dots +
x_n
\begin{bmatrix}
    a_{1n}\\
    a_{mn}
\end{bmatrix}
$$


\subsection{Column-Row Expansion}

\begin{align*}
    AB &=
\left[ \begin{array}{c|c}
    2 & 3\\
    1 & 4\\
\end{array} \right]
\left[ \begin{array}{ccc}
    1 & 4 & 6\\
    \hline
    6 & 3 & 5
\end{array} \right]\\
    &=
    \begin{bmatrix}2\\1\end{bmatrix}
    \begin{bmatrix}1 & 4 & 6\end{bmatrix}
        +
    \begin{bmatrix}3\\4\end{bmatrix}
    \begin{bmatrix}6 & 3 & 5\end{bmatrix}
    \\
    &=
    \begin{bmatrix}2 & 8 & 12\\1 & 4 & 6\end{bmatrix}
        +
    \begin{bmatrix}18 & 9 & 15\\24 & 12 & 20\end{bmatrix}
    \\
    &=
    \begin{bmatrix}20&17&27\\25&16&26\end{bmatrix}
\end{align*}


\subsection{Trace}

$\tr(A)$ of a square matrix $A$ is defined by the sum of the entries on the main diagonal of $A$.\\
\begin{align*}
    \tr(AB) &\neq \tr(A)\tr(B)\\
    \tr(A^T) &= \tr(A)\\
    \tr(cA) &= c\tr(A)
\end{align*}


\subsection{Nonsingular Matrix}

A matrix that is invertible.


\subsection{Determinant and Cofactor Expansion}

The determinant of a $2\times2$ matrix $\begin{bmatrix}a&b\\c&d\end{bmatrix}$ is $ad - bc$.\\
    The determinant of a square matrix can be determined by choosing a row or column and multiplying each entry by the corresponding cofactor. For instance, the determinant of the matrix $A$ can be found by choosing the first row, $\det(A) = a_{11}C_{11} + \dots + a_{1n}C_{1n}$.


\subsection{$2\times2$ Inverse}

The inverse of a $2\times2$ matrix is $\displaystyle\frac{1}{ad - bc} \begin{bmatrix}d&-b\\-c&a\end{bmatrix}$.


\subsection{Minor}

The minor of entry $A_{ij}$ is the determinant of the submatrix $A$ with row $i$ and column $j$ deleted.


\subsection{Cofactor}

The cofactor of entry $A_{ij}$ is the minor of entry $A_{ij} * (-1)^{i + j}$. If $i + j$ is even, the minor retains its sign.


\subsection{Row Equivalency}

Two matrices are row equivalent if either can be obtained from the other by a sequence of elementary row operations.\\
If $A$ and $B$ are row equivalent, and if $B$ and $C$ are row equivalent, then $A$ and $C$ are row equivalent.


\subsection{Elementary Matrix}

A matrix that can be obtained from an identity matrix by a single elementary row operation.


\subsection{Adjoint Matrix}

An adjoint of the matrix $A$ is the transpose of a matrix where each entry is the corresponding cofactor from $A$. $(\adj(A)_{ij}) = C_{ji}$.


\subsection{Eigenvectors and Eigenvalues}

Any vector $x$ that holds $Ax = \lambda x$ for the given $\lambda$ is an eigenvector. The scalar $\lambda$ is an eigenvalue. $x$ is said to be an eigenvector corresponding to $\lambda$.


\section{Equivalence Theorem}

If $A$ is an $n \times n$ matrix, then the following statements are equivalent. That is, if one is true, the rest is true, as they are logically equivalent.
\begin{itemize}
    \item $A$ is invertible.
    \item $Ax = 0$ has only the trivial solution.
    \item The reduced row echelon form of $A$ is $I_n$.
    \item $A$ is expressible as a product of elementary matrices. $A = E_nE_{n-1}\dots E_1I_n$.
    \item $Ax = b$ has exactly one solution for every $n \times 1$ matrix $b$. $x = A^{-1}b$.
    \item $\det(A) \neq 0$.
    \item $\lambda = 0$ is not an eigenvalue of $A$.
\end{itemize}
Extra:
\begin{itemize}
    \item If $A$ is a singular (non-invertible) square matrix then the linear system $Ax = 0$ has infinitely many solutions.
\end{itemize}


\section{Determinant Properties}


\subsection{Adjoint Matrices}

We know the following:
\begin{align*}
    A\adj(A) &= \det(A)I\\
    \adj(A) &= A^{-1}\det(A)I
\end{align*}
We can then find the determinant of the adjoint of a matrix in terms of the determinant of the original matrix.
\begin{align*}
    A &= \adj(B)\\
    A &= B^{-1}\det(B)I\\
    \det(A) &= \det(B^{-1}) \det(\det(B)) \det(I)\\
    \det(A) &= \det(B)^{-1} \det(B)^n 1\\
    \det(A) &= \det(B)^{n-1}
\end{align*}


\section{Theorems}


\subsection{Free Variable Theorem and Homogeneous Linear System Theorems}

These theorems apply only to homogeneous linear systems (HLS).
\begin{itemize}
    \item An HLS in reduced row echelon form with $n$ unknowns and $r$ nonzero rows (thus, $r$ leading 1s) has $n - r$ free variables.
    \item An HLS with more unknowns than equations has infinitely many solutions.
    \item An HLS of $n$ equations with $n$ leading 1s in reduced row echelon form has only the trivial solution (see equivalence theorem).
\end{itemize}


\subsection{Inverse Theorems}

\begin{itemize}
    \item If a square matrix has a zero row or column, it is singular (not invertible). The definition of an invertible matrix is $AA^{-1} = I$. If a zero row exists, the product could not reduce down to the identity matrix.
    \item If $B$ and $C$ are both inverses of $A$ then $B = C$. An invertible matrix has only one inverse.
    \item $(AB)^{-1} = B^{-1}A^{-1}$. Note the reverse order. This means that to cancel out the matrix $A$ in $ABC = I$ we would need to multiply $A^{-1}$ on the LHS of each side. $A^{-1}ABC = A^{-1}I \Rightarrow BC = A^{-1}I$.
    \item If a product of matrices is non-invertible (singular) then at least one of the factors must also be singular.
    \item $(A^{-1})^{-1} = A$
    \item $(A^n)^{-1} = A^{-n} = (A^{-1})^n$
    \item $(kA)^{-1} = k^{-1}A^{-1}$ with $k$ being a scalar, thus $k^{-1} = \frac{1}{k}$.
    \item If $AB$ is invertible then $A$ and $B$ must also both be invertible.
    \item If $A$ and/or $B$ are not invertible, then neither is $AB$.
    \item $A^{-1} = \frac{1}{\det(A)} \adj(A)$
\end{itemize}


\subsection{Transpose Theorems}

\begin{itemize}
    \item $(A^T)^T = A$
    \item $(A \pm B)^T = A^T \pm B^T$
    \item $(kA)^T = kA^T$
    \item $(AB)^T = B^TA^T$ (note the reverse order)
    \item If $A$ is invertible then $A^T$ is also invertible. $(A^T)^{-1} = (A^{-1})^T$.
\end{itemize}


\subsection{Elementary Theorems}

\begin{itemize}
    \item Every elementary matrix is invertible.
    \item The inverse of any elementary matrix is also an elementary matrix.
    \item The product of matrix $A$ and an elementary matrix is the matrix that results when the corresponding row operation is performed on $A$.
    \item The inverse of a matrix can be obtained by performing the same row operations that reduce the matrix to the identity to an identity matrix. If $E_1E_0A = I$ then $A^{-1} = E_1E_0I$.
    \item If $E$ is produced from multiplying a row by $k$ then $\det(E) = k$. See Determinant Theorems below.
    \item If $E$ is produced by interchanging two rows then $\det(E) = -1$.
    \item If $E$ is produced by adding a multiple of a row to another then $\det(E) = 1$.
\end{itemize}


\subsection{Diagonal Matrix Theorems}

\begin{itemize}
    \item Invertible if all of its diagonal entries are nonzero.
    \item Powers of diagonal matrices (including their inverse) are easy to compute, $\begin{bmatrix}a&0\\0&b\end{bmatrix}^{-1} = \begin{bmatrix}\frac{1}{a} & 0\\0 & \frac{1}{b}\end{bmatrix}$.
    \item If $D$ is a diagonal matrix then the product $AD$ can be found by multiplying the entry of each column in $A$ by the corresponding diagonal entry (entries in first column of $A$ multiplied by $D_{11}$. The product of $DA$ can be found by multiplying the entry of each row in $A$ by the corresponding diagonal entry (entries in first row of $A$ multiplied by $D_{11}$.
        \begin{align*}
            \begin{bmatrix}d_1&0\\0&d_2\end{bmatrix}\begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix} &= \begin{bmatrix}d_1a_{11}&d_1a_{12}\\d_2a_{21}&d_2a_{22}\end{bmatrix}\\
            \begin{bmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{bmatrix}\begin{bmatrix}d_1&0\\0&d_2\end{bmatrix} &= \begin{bmatrix}d_1a_{11}&d_2a_{12}\\d_1a_{21}&d_2a_{22}\end{bmatrix}
        \end{align*}
\end{itemize}


\subsection{Triangular Matrix Theorems}

\begin{itemize}
    \item The transpose of a triangular matrix is the opposite kind of triangular matrix (upper to lower and vise-versa).
    \item The product of same-side triangular matrices is the same side (product of upper triangular matrices is upper triangular).
    \item A triangular matrix is invertible if its diagonal entries are all nonzero.
    \item The inverse of a triangular matrix is the same side.
    \item The determinant of a triangular matrix is the product of the entries on the main diagonal, $\det(A) = a_{11}a_{22}\dots a_{nn}$.
\end{itemize}


\subsection{Symmetric Theorems}

If $A$ and $B$ are symmetric with the same size and $k$ is a scalar constant then the following theorems hold.
\begin{itemize}
    \item $A \pm B$ is symmetric. This holds for skew-symmetric matrices.
    \item $kA$ is symmetric. This holds for skew-symmetric matrices.
    \item $AB$ is symmetric if $A$ and $B$ commute. This does not hold for skew-symmetric matrices.
    \item If $A$ is invertible then $A^{-1}$ is symmetric. This holds for skew-symmetric matrices.
    \item If $A$ is invertible then $AA^T$ and $A^TA$ are also invertible.
    \item Every square matrix can be expressed as the sum of a symmetric and skew-symmetric matrix.
\end{itemize}
The following matrices are always symmetric even if $A$ is not.
\begin{itemize}
    \item $A + A^T$ (but this does not hold for $A - A^T$).
    \item $AA^T$
\end{itemize}


\subsection{Determinant Theorems}

\begin{itemize}
    \item If $A$ is a square matrix with a row or columns of zero, then $\det(A) = 0$.
    \item $\det(A) = \det(A^T)$
    \item $\det(B) = k\det(A)$ when a single row or single column of $A$ is multiplied by $k$.
    \item $\det(B) = -\det(A)$ when two rows or two columns of $A$ are interchanged.
    \item $\det(B) = \det(A)$ when a multiple of a row or column in $A$ is added to another.
    \item If a square matrix has two proportional rows or two proportional columns then $\det(A) = 0$.
    \item $\det(kA) = k^n\det(A)$ where $n$ is the number of rows in $A$.
    \item $\det(A + B) \neq \det(A) + \det(B)$
    \item $\det(C) = \det(A) + \det(B)$ if $A$, $B$, and $C$ differ only in a single row $r$ and that the $r$th row of $C$ can be produced by adding the $r$th rows of $A$ and $B$.
    \item $\det(AB) = \det(A)\det(B)$
    \item $\det(A^{-1}) = \frac{1}{\det(A)}$
\end{itemize}
Examples:
\begin{align*}
    &\begin{vmatrix}3&-6&9\\-2&7&-2\\0&1&5\end{vmatrix}&\text{The determinant of this matrix is 33}\\
        =3&\begin{vmatrix}1&-2&3\\-2&7&-2\\0&1&5\end{vmatrix}&\text{The determinant of this matrix 11}
\end{align*}
If the lower matrix is $B$ and the upper one is $A$ then $A$ had a row multiplied by $3$ and thus $\det(A) = 3\det(B)$.


\section{Eigenstuff}


\subsection{Characteristic Equation}

$Ax = \lambda x$ can be rewritten as $Ax = \lambda Ix \Rightarrow (\lambda I - A)x = 0$.\\
$\lambda$ is an eigenvalue of $A$ iff it satisfies $\det(\lambda I - A) = 0$.


\subsection{Bases}

The bases are all the distinct eigenvectors, the collection of basis (singular) corresponding to each eigenvalue.


\subsection{Theorems}

\begin{itemize}
    \item $\det(A - \lambda I) = 0$ means there are infinitely many solutions to $Ax - \lambda x = 0$.
    \item $\det(A - \lambda I) \neq 0$ means there is exactly one solution, the matrix is invertible (see equivalence theorem).
    \item The eigenvalues of $A$ are the solutions to $\det(\lambda I - A) = 0$.
    \item The characteristic equation of an $n\times n$ matrix has at most $n$ distinct eigenvalues.
    \item The eigenvalues of a triangular matrix are the entries on its main diagonal.
\end{itemize}


\subsection{Example}

\begin{align*}
    A &=
\begin{bmatrix}
    4 & 0 & -1\\
    0 & 3 & 0\\
    1 & 0 & 2
\end{bmatrix}
\\
    \det(\lambda I - A) &= 0\\
\begin{vmatrix}
    \lambda - 4 & 0 & 1\\
    0 & \lambda - 3 & 0\\
    -1 & 0 & \lambda - 2
\end{vmatrix} &= 0
\\
    (\lambda - 3)\left((\lambda - 4)(\lambda - 2) + 1\right) &= 0\\
    (\lambda - 3)(\lambda^2 - 6\lambda + 9) &= 0\\
    (\lambda - 3)(\lambda^2 - 3\lambda - 3\lambda + 9) &= 0\\
    (\lambda - 3)(\lambda(\lambda - 3) - 3(\lambda - 3)) &= 0\\
    (\lambda - 3)^3 &= 0\\
    \lambda &= 3
\end{align*}
Then find the eigenvectors for $\lambda = 3$.
\begin{align*}
    (\lambda I - A)x &= 0\\
    \begin{bmatrix}-1&0&1\\0&0&0\\-1&0&1\end{bmatrix} \begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix} &= \begin{bmatrix}0\\0\\0\end{bmatrix}\\
    \begin{bmatrix}1&0&-1\\-1&0&1\\0&0&0\end{bmatrix} \begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix} &= \begin{bmatrix}0\\0\\0\end{bmatrix}\\
    \begin{bmatrix}1&0&-1\\0&0&0\\0&0&0\end{bmatrix} \begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix} &= \begin{bmatrix}0\\0\\0\end{bmatrix}
\end{align*}
Parameterize the result of the reduced row echelon form.
\begin{align*}
    x_2 &= s\\
    x_3 &= t\\
    x_1 - x_3 &= 0\Rightarrow x_1 - t = 0 \Rightarrow x_1 = t\\
    x &= \begin{bmatrix}t\\s\\t\end{bmatrix} = \begin{bmatrix}t\\0\\t\end{bmatrix} + \begin{bmatrix}0\\s\\0\end{bmatrix}\\
    x &= t\begin{bmatrix}1\\0\\1\end{bmatrix} + s\begin{bmatrix}0\\1\\0\end{bmatrix}
\end{align*}
Basis for $\lambda = 3$ are $\begin{bmatrix}1\\0\\1\end{bmatrix}$ and $\begin{bmatrix}0\\1\\0\end{bmatrix}$.


\end{document}
